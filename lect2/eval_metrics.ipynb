{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b89b7da",
   "metadata": {},
   "source": [
    "# Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f635c",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "\n",
    "### Accuracy\n",
    "The proportion of correctly predicted instances out of the total instances.\n",
    "\n",
    " $\\text{Accuracy} =\\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predicted):\n",
    "    return sum([l == p for l, p in zip(labels, predicted)]) / len(labels)\n",
    "\n",
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "accuracy(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551cbb3",
   "metadata": {},
   "source": [
    "Accuracy is not a good metrix if classes are heavily umbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13222ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "predicted = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "accuracy(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54da7bf",
   "metadata": {},
   "source": [
    "The following metrics are defined for two class problems, but can be extended to multiple class problems. We will add the class to the variable name\n",
    "- You should use them if you want a custom analysis for each of the classes\n",
    "\n",
    "### Precision\n",
    "The proportion of true positive instances out of the total instances predicted as positive.\n",
    "\n",
    "$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precission(labels, predicted, klass):\n",
    "    c = sum([p == klass for p in predicted])\n",
    "    if c > 0:   \n",
    "        return sum([l == p and l == klass for l, p in zip(labels, predicted)]) / c\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "precission(labels, predicted, 1), precission(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0df81",
   "metadata": {},
   "source": [
    "Precision for a single class should be analyzed with care:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 1, 1, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "\n",
    "precission(labels, predicted, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precission(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989413c3",
   "metadata": {},
   "source": [
    "### Recall (Sensitivity or True Positive Rate)\n",
    "The proportion of true positive instances out of the total actual positive instances.\n",
    "\n",
    "$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(labels, predicted, klass):\n",
    "    return sum([l == p and l == klass for l, p in zip(labels, predicted)]) / sum([p==klass for p in labels])\n",
    "\n",
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "recall(labels, predicted, 1), recall(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 1, 1, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "\n",
    "recall(labels, predicted, 1), recall(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bb297",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "The harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "$\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ba64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(labels, predicted, klass):\n",
    "    p = precission(labels, predicted, klass)\n",
    "    r = recall(labels, predicted, klass)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "f1_score(labels, predicted, 1), f1_score(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f576b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 1, 1, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 0, 0, 0, 1, 1]\n",
    "\n",
    "f1_score(labels, predicted, 1), f1_score(labels, predicted, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deefb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalance case\n",
    "labels =    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "predicted = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "f1_score(labels, predicted, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16088855",
   "metadata": {},
   "source": [
    "### ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "The AUC represents the degree or measure of separability, indicating how well the model distinguishes between classes.\n",
    "\n",
    "In different classification models, one can set a threshold that is used to determine if an object belongs to a class or the other. For example, consider that the result of a classification algorithm returns some sort of \"probability\" of one particular class. Once we set a threshold value, we turned the probability into classes, and the metrics can be calculated.\n",
    "\n",
    "In order to use the most common ROC curve, we need to introduce another metric, the False Positive Rate (FPR), which is better if closer to zero.\n",
    "\n",
    "$\\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$\n",
    "\n",
    "This is used together with the True Positive Rate, or recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fpr(labels, predicted, klass):\n",
    "    return sum([l != klass and p == klass for l, p in zip(labels, predicted)]) / sum([p!=klass for p in labels])\n",
    "\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "predicted_probs = [0.3, 0.4, 0.1, 0.6, 0.0, 0.95, 0.4, 0.6, 0.3, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7204d6",
   "metadata": {},
   "source": [
    "Once we set a given threshold, for example 0.5, then we can turn predicted_probs intro predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa341d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "predicted = [0 if p < threshold else 1 for p in predicted_probs]\n",
    "print(labels)\n",
    "print(predicted)\n",
    "recall(labels, predicted, 1), fpr(labels, predicted, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c4c11",
   "metadata": {},
   "source": [
    "Lets change explore what happens for all different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfa250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(predicted_probs)\n",
    "all_r = []\n",
    "all_f = []\n",
    "print()\n",
    "print(\"thr, TPR, FPR\")\n",
    "for threshold in np.arange(0, 1.1, 0.1):\n",
    "    predicted = [0 if p < threshold else 1 for p in predicted_probs]\n",
    "    r, f = recall(labels, predicted, 1), fpr(labels, predicted, 1)\n",
    "    all_r.append(r)\n",
    "    all_f.append(f)\n",
    "    print(f\"{threshold:.1f}, {r:.2f}, {f:.2f}, {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_f, all_r, marker='o')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6205f",
   "metadata": {},
   "source": [
    "A ROC curve shows the tradeoff of using a threshold in a particular method. \n",
    "- Good methods allows to increase the TPR without sacrifying too much the FPR\n",
    "- Bad methods can only increase the TPR by degrading proportionally the FPR\n",
    "\n",
    "Lets put all the code in a function in order to test all this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ROC_report(labels, predicted_probs):\n",
    "    all_r = []\n",
    "    all_f = []\n",
    "    print()\n",
    "    print(\"thr, TPR, FPR\")\n",
    "    for threshold in np.arange(0, 1.1, 0.1):\n",
    "        predicted = [0 if p < threshold else 1 for p in predicted_probs]\n",
    "        r, f = recall(labels, predicted, 1), fpr(labels, predicted, 1)\n",
    "        all_r.append(r)\n",
    "        all_f.append(f)\n",
    "        print(f\"{threshold:.1f}, {r:.2f}, {f:.2f}, {predicted}\")\n",
    "\n",
    "    plt.plot(all_f, all_r, marker='o')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517dee0b",
   "metadata": {},
   "source": [
    "In a random classifier, the predicted probability is not related to real class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "\n",
    "labels = [0 for _ in range(n_points//2 )] + [1 for _ in range(n_points//2 )]\n",
    "predicted_probs = np.random.uniform(0, 1, (n_points,))\n",
    "create_ROC_report(labels, predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "\n",
    "labels = [0 for _ in range(n_points//2 )] + [1 for _ in range(n_points//2 )]\n",
    "predicted_probs = np.hstack([\n",
    "    np.random.uniform(0, 0.49, (n_points//2,)),\n",
    "    np.random.uniform(0.51, 1, (n_points//2,))\n",
    "])\n",
    "\n",
    "create_ROC_report(labels, predicted_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4b6f4",
   "metadata": {},
   "source": [
    "The area below the ROC-curve (AUC-ROC) is minimal in the random case and maximal (equal to one) for the perfect case, so it can be used as a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bb736",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "A matrix showing the counts of true positive, true negative, false positive, and false negative predictions, useful for a more detailed performance analysis.\n",
    "\n",
    "In many classification problems, specially those with more than two classes, the number of errors might be less important than the distribution of errors. This is totally missed by global metrics like the accuracy.\n",
    "\n",
    "Lets look this threee examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 1, 1, 1, 1, 0, 0]\n",
    "accuracy(labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b971154",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "accuracy(labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "accuracy(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73004de4",
   "metadata": {},
   "source": [
    "In all the examples the classifier missclassify half of the objects, but if you want to improve the result you need to know where are the errors located. This is shown in the confussion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 1, 1, 1, 1, 0, 0]\n",
    "confusion_matrix(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e3ef2",
   "metadata": {},
   "source": [
    "In a confusion matrix, each object in counted in the row of its real class, and the column of its assigned class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "confusion_matrix(labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d80749",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "predicted = [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "confusion_matrix(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7495ae",
   "metadata": {},
   "source": [
    "With more than two classes, the interpretation can be even more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =    [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
    "predicted = [0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 1]\n",
    "confusion_matrix(labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ef43f",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n",
    "\n",
    "$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e35b70",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE): The average of the squared differences between predicted and actual values.\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "     $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0bdef",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE): The square root of the mean squared error.\n",
    "\n",
    "$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}$\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1df69",
   "metadata": {},
   "source": [
    "### R-squared (Coefficient of Determination)\n",
    "The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "$R^2 = 1 - \\frac{\\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}{\\sum_{i=1}^{n} \\left( y_i - \\bar{y} \\right)^2}$\n",
    "\n",
    "where $\\bar{y}$ is the mean of the actual values.\n",
    "\n",
    "The coefficient of determination, denoted as R2, is a statistical measure that explains the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It provides an indication of the goodness of fit of a model. An R2 value of 1 indicates that the regression predictions perfectly fit the data.\n",
    "- It ranges from 0 to 1.\n",
    "- R2 of 0 means that the dependent variable cannot be predicted from the independent variable(s).\n",
    "- R2 of 1 means that the dependent variable can be predicted without error from the independent variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31416)  \n",
    "values = np.random.randint(1, 100, 10)\n",
    "predicted = values + np.random.normal(0, 10, 10)  \n",
    "values, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38679e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(values, predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_of_data = np.sum((values - np.mean(values))**2)\n",
    "variance_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adac3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_of_model = np.sum((values - predicted)**2)\n",
    "variance_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - (variance_of_model / variance_of_data)\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53887544",
   "metadata": {},
   "source": [
    "Adding different levels of noise makes the regression worse, and it is reflected in R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise in range(0, 40, 3):\n",
    "    predicted = values + np.random.normal(0, noise, 10)  \n",
    "    variance_of_data = np.sum((values - np.mean(values))**2)\n",
    "    variance_of_model = np.sum((values - predicted)**2)\n",
    "    R2 = 1 - (variance_of_model / variance_of_data)\n",
    "    print(noise, R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028f128",
   "metadata": {},
   "source": [
    "#### R2 or MSE?\n",
    "\n",
    "Advantages of R2:\n",
    "\n",
    "- Can calculate statistical significance using a test.\n",
    "\n",
    "- Scale-Free Comparison: R2 is scale-free.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "- Mathematical Simplicity.\n",
    "\n",
    "- Faster calculation.\n",
    "\n",
    "\n",
    "Choosing Between R2 and MSE:\n",
    "\n",
    "- Model Assessment: Use R2 when you want to assess how well your model explains the variance in the dependent variable relative to a baseline model. It provides a holistic view of model performance in terms of variance explained.\n",
    "\n",
    "- Model Optimization: Use MSE (or related metrics like RMSE) as a loss function during model training and optimization. MSE directly penalizes larger errors, making it suitable for training models to minimize prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716e21d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
