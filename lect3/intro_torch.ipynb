{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239ed5fb",
   "metadata": {},
   "source": [
    "# Introduction to Torch\n",
    "- Deep learning library developed by Meta (Formerly Facebook)\n",
    "- Is a key component of PyTorch ecosystem\n",
    "\n",
    "Similarities with Autograd:\n",
    "- Automatic differentiation\n",
    "- Easy of use\n",
    "- Pythonic integration\n",
    "- Dynamic computational graphs\n",
    "- Gradient computation and optimization\n",
    "\n",
    "Differences with AutoGrad:\n",
    "- Based on **tensors** (multidimensional matrixes) instead of **values**\n",
    "- Easy to execute on GPUs of different vendors\n",
    "- Lots of components and tools integrated\n",
    "\n",
    "## A simple example in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca96a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()\n",
    "\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('b', b.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6ba12",
   "metadata": {},
   "source": [
    "## A neural network in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor([\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, -1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "])\n",
    "ys = torch.tensor([1.0, -1.0, -1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron, three inputs\n",
    "W = torch.randn((3, 1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969df59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation of the neuron on the four inputs\n",
    "xs @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now add the bias\n",
    "b = torch.randn((1,))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f0b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# activation of the first neuron for the four inputs\n",
    "xs @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add more neurons\n",
    "W = torch.randn((3, 4))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7767e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what is now the product?\n",
    "xs @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00197cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs -> objects x features\n",
    "# W -> weights x neurons\n",
    "# xs @ W -> objects x neurons, each cell contains sum(features*weights)\n",
    "xs.shape, W.shape, (xs@W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74238e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the bias is added per neuron, after xs * W\n",
    "b = torch.randn((1,4))\n",
    "act = xs @ W + b\n",
    "display(xs @ W)\n",
    "display(b)\n",
    "display(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d121ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now, it is time to evaluate the activation function on the activation\n",
    "(xs @ W + b).tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec589f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df7eca37",
   "metadata": {},
   "source": [
    "As a summary, a fully connected layer is formed by:\n",
    "- A weight matrix that has one row per input value, and a column per output value\n",
    "- A bias vector, that has a single row and a column per neuron\n",
    "- An activation function\n",
    "\n",
    "Note: All the elements here are differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to the original problem\n",
    "display(xs)\n",
    "display(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ffc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have three features and one output, so we need one output neuron\n",
    "W = torch.randn((3, 1))\n",
    "b = torch.randn((1, 1))\n",
    "\n",
    "out = (xs @ W + b).tanh()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets calculate the loss\n",
    "loss = torch.sum((out - ys)**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e4341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uppps, it should not be that large!\n",
    "out-ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb882bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, ys.shape, (out-ys).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one solution is remove the last dimension from out\n",
    "out.shape, out.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d48a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.squeeze() - ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f495770",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.sum((out.squeeze() - ys)**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since everything is differentiable, we can backpropagate the loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In torch, we need to warn the framework about the tensors we need to calculate the grad\n",
    "\n",
    "# network definition, seed controled\n",
    "g = torch.Generator().manual_seed(31416)\n",
    "W = torch.randn((3, 1), generator=g, requires_grad=True)\n",
    "b = torch.randn((1, 1), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    out = (xs @ W + b).tanh()\n",
    "    loss = torch.mean((out.squeeze() - ys)**2)\n",
    "#     print(loss.item())\n",
    "\n",
    "    # backward pass, need to remove the gradients\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    W.data += -learning_rate * W.grad\n",
    "    b.data += -learning_rate * b.grad\n",
    "print(\"Final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f76828",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.data, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(31416)\n",
    "W = torch.randn((3, 1), generator=g, requires_grad=True)\n",
    "b = torch.randn((1, 1), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41857d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let simplify the code\n",
    "learning_rate = 0.1\n",
    "parameters = [W, b]\n",
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    out = (xs @ W + b).tanh()\n",
    "    loss = torch.mean((out.squeeze() - ys)**2)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass, need to remove the gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "print(\"Final loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e78fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.data, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809efac",
   "metadata": {},
   "source": [
    "## Solving moon problem, using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfadc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets transform them to tensors, and keep only the first to simplify the explanations\n",
    "xs = torch.tensor(X, dtype=torch.float32, device=device)[:5]\n",
    "ys = torch.tensor(y, device=device)[:5]\n",
    "xs.shape, ys.shape\n",
    "ys.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad71c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets create a neural network having two hidden layers, with 4 and 5 neurons\n",
    "# Since it is a classification problem, we will use one output layer per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(31416+2)\n",
    "W1 = torch.randn((2, 4), generator=g, requires_grad=True, device=device)\n",
    "b1 = torch.randn((1, 4), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "W2 = torch.randn((4, 5), generator=g, requires_grad=True, device=device)\n",
    "b2 = torch.randn((1, 5), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "W3 = torch.randn((5, 2), generator=g, requires_grad=True, device=device)\n",
    "b3 = torch.randn((1, 2), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "parameters = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d16bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "l1_out = (xs @ W1 + b1).tanh()\n",
    "l2_out = (l1_out @ W2 + b2).tanh()\n",
    "l3_out = (l2_out @ W3 + b3)\n",
    "l3_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b55ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_out.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8abcef",
   "metadata": {},
   "source": [
    "We want the output of the network to be class probabilities, so we need to transform the outputs. To be considered as a probability, values must between 0 and 1, and they must add to 1.\n",
    "\n",
    "First, lets apply the exponencial function, to turn all values positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc71991",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_out.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fcbd2",
   "metadata": {},
   "source": [
    "Now, re-scale the values so they add 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c8d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value = l3_out.exp()\n",
    "(value / value.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c27dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the cause of the error\n",
    "value.shape, value.sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85007135",
   "metadata": {},
   "source": [
    "The problem happens because of the way broadcast is performed:\n",
    "- Align the dimension from right to left\n",
    "- Fill missing dimensions with 1\n",
    "- Dimensions are compatible if they have the same size, or all but one contains value 1\n",
    "- Broadcast the values in the values in the vector with 1 as necessary\n",
    "\n",
    "Examples:\n",
    "- 5x1, 1x5 -> 5x5\n",
    "- 6x3, 3 -> 6x3 , 1x3 -> 6x3\n",
    "\n",
    "In the line with the error, torch try to broadcast 5x2 and 5 -> 5x2, 1x5, and they are not compatible.\n",
    "\n",
    "There are some solutions. Here we use the unsqueeze() method that adds a dimension to the tensor, in the **dim** position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = l3_out.exp()\n",
    "probs = (value / value.sum(dim=1).unsqueeze(dim=1))\n",
    "probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f58249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety check\n",
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968c4ff",
   "metadata": {},
   "source": [
    "Now we have probabilities as the output of the network. How to create a loss function, using this probabilities.\n",
    "\n",
    "One commonly used idea is to use the maximal likelihood estimation. It estimates the quality of the result by multiplying the probabilities obtained for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(probs)\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case\n",
    "probs[0, 0], probs[1, 1], probs[2, 0], probs[3, 1], probs[4,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc96e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to obtain this probabilities using tensor manipulation: using a type of indexing\n",
    "probs[torch.arange(len(ys)), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58282c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets multiply them\n",
    "likelihood = probs[torch.arange(len(ys)), ys].prod()\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512039d",
   "metadata": {},
   "source": [
    "This is a quite small number, and we only have 5 results ... imagine when we have 100s\n",
    "\n",
    "Solution, use the logarithms, and average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255acccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = probs[torch.arange(len(ys)), ys].log().mean()\n",
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45513f5e",
   "metadata": {},
   "source": [
    "Since likelihood is higher (so log_likelihood) in better results, and we need a loss function, we will invert the sign.\n",
    "\n",
    "**Note:** All performed operations are derivables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677498b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = - log_likelihood\n",
    "nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbeaef1",
   "metadata": {},
   "source": [
    "Lets put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "ys = torch.tensor(y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b309845",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(31416+2)\n",
    "W1 = torch.randn((2, 4), generator=g, requires_grad=True, device=device)\n",
    "b1 = torch.randn((1, 4), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "W2 = torch.randn((4, 5), generator=g, requires_grad=True, device=device)\n",
    "b2 = torch.randn((1, 5), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "W3 = torch.randn((5, 2), generator=g, requires_grad=True, device=device)\n",
    "b3 = torch.randn((1, 2), generator=g, requires_grad=True, device=device)\n",
    "\n",
    "parameters = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c4107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    l1_out = (xs @ W1 + b1).tanh()\n",
    "    l2_out = (l1_out @ W2 + b2).tanh()\n",
    "    l3_out = (l2_out @ W3 + b3)\n",
    "    value = l3_out.exp()\n",
    "    probs = (value / value.sum(dim=1).unsqueeze(dim=1))\n",
    "    log_likelihood = probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    loss = -log_likelihood\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item()}\")\n",
    "    \n",
    "    # backward pass, need to remove the gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the classifier accuracy\n",
    "probs[torch.arange(len(ys)), ys] > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a6633",
   "metadata": {},
   "source": [
    "The classifier commits no mistakes, so it's accuracy is 1.0. \n",
    "\n",
    "Lets plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584beabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "Xmesh[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7822b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs_full = torch.tensor(Xmesh, device=device).float()\n",
    "l1_out = (xs_full @ W1 + b1).tanh()\n",
    "l2_out = (l1_out @ W2 + b2).tanh()\n",
    "l3_out = (l2_out @ W3 + b3).tanh()\n",
    "value = l3_out.exp()\n",
    "probs = (value / value.sum(dim=1).unsqueeze(dim=1))\n",
    "probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.max(probs, dim=1).indices\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c23c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([s.data > 0 for s in scores.cpu()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc39730",
   "metadata": {},
   "source": [
    "### Use modules in torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ca046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "xs = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "ys = torch.tensor(y, device=device)\n",
    "\n",
    "# Using torch fully connected layers\n",
    "l1 = nn.Linear(in_features=2, out_features=4, device=device)\n",
    "l2 = nn.Linear(in_features=4, out_features=5, device=device)\n",
    "l3 = nn.Linear(in_features=5, out_features=2, device=device)\n",
    "parameters = list(l1.parameters()) + list(l2.parameters()) + list(l3.parameters())\n",
    "\n",
    "x = l1(xs).tanh()\n",
    "x = l2(x).tanh()\n",
    "x = l3(x)\n",
    "\n",
    "# soft-max layer + entropy loss = cross_entropy\n",
    "loss = F.cross_entropy(x, ys)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    x = l1(xs).tanh()\n",
    "    x = l2(x).tanh()\n",
    "    x = l3(x)\n",
    "    loss = F.cross_entropy(x, ys)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item()}\")\n",
    "    \n",
    "    # backward pass, need to remove the gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a06513",
   "metadata": {},
   "source": [
    "Lets put in into a more compacted version, using Sequential, and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "ys = torch.tensor(y, device=device)\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4, device=device), nn.Tanh(),\n",
    "    nn.Linear(4, 5, device=device), nn.Tanh(),\n",
    "    nn.Linear(5, 2, device=device)\n",
    ")\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Forward pass\n",
    "    x = model(xs)\n",
    "    loss = F.cross_entropy(x, ys)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d35a0",
   "metadata": {},
   "source": [
    "Here we used an optimizer, that performs the update operation faster and smartly, together with the zero_grad() operation.\n",
    "\n",
    "There is other way to create the network, subclassing the Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9510faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 4, device=device)\n",
    "        self.layer2 = nn.Linear(4, 5, device=device)\n",
    "        self.layer3 = nn.Linear(5, 2, device=device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = self.layer1(xs).tanh()\n",
    "        x = self.layer2(x).tanh()\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "xs = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "ys = torch.tensor(y, device=device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(xs)\n",
    "    loss = F.cross_entropy(outputs, ys)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7be8aa",
   "metadata": {},
   "source": [
    "# Some unfrequent uses of torch autograd\n",
    "\n",
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class KMeansTorch:\n",
    "    def __init__(self, n_clusters, max_iters=100, tol=1e-4, lr=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Randomly initialize the centroids as k random samples from X\n",
    "        indices = torch.randperm(X.size(0))[:self.n_clusters]\n",
    "        self.centroids = X[indices]\n",
    "        self.centroids.requires_grad = True  # Enable gradients for centroids\n",
    "\n",
    "        optimizer = torch.optim.Adam([self.centroids], lr=self.lr)\n",
    "\n",
    "        for i in range(self.max_iters):\n",
    "            # Assign each point to the nearest centroid\n",
    "            distances = torch.cdist(X, self.centroids, p=2)\n",
    "            cluster_assignments = torch.argmin(distances, dim=1)\n",
    "\n",
    "            # Compute the loss (sum of squared distances)\n",
    "            loss = 0\n",
    "            for k in range(self.n_clusters):\n",
    "                loss += ((X[cluster_assignments == k] - self.centroids[k]) ** 2).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Check for convergence (if centroids do not change significantly)\n",
    "            with torch.no_grad():\n",
    "                if i > 0 and prev_loss - loss.item() < self.tol:\n",
    "                    break\n",
    "                prev_loss = loss.item()\n",
    "\n",
    "        self.labels_ = cluster_assignments.detach()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = torch.cdist(X, self.centroids, p=2)\n",
    "        return torch.argmin(distances, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate 2D Gaussian data\n",
    "def generate_2d_gaussian(mean, cov, num_samples):\n",
    "    return np.random.multivariate_normal(mean, cov, num_samples)\n",
    "\n",
    "# Means and covariances for the Gaussians\n",
    "means = [[2, 2], [8, 3], [5, 7]]\n",
    "covs = [[[1, 0.5], [0.5, 1]], [[1, -0.7], [-0.7, 1]], [[1, 0.3], [0.3, 1]]]\n",
    "\n",
    "# Generate samples\n",
    "num_samples = 100\n",
    "data = np.vstack([generate_2d_gaussian(mean, cov, num_samples) for mean, cov in zip(means, covs)])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X = torch.tensor(data, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated data points\n",
    "plt.scatter(data[:, 0], data[:, 1], s=10)\n",
    "plt.title(\"Generated 2D Gaussian Data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242de17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeansTorch(n_clusters=3, max_iters=1000, lr=0.01)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustered data points\n",
    "plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_.cpu(), s=10, cmap='viridis')\n",
    "plt.scatter(kmeans.centroids[:, 0].detach().cpu().numpy(), \n",
    "            kmeans.centroids[:, 1].detach().cpu().numpy(), \n",
    "            s=100, c='red', label='Centroids')\n",
    "plt.title(\"KMeans Clustering with PyTorch\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ef96c",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(100, 5)\n",
    "X = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Initialize the principal component vector (random)\n",
    "w = torch.randn(X.size(1), requires_grad=True)\n",
    "\n",
    "# Learning rate and iterations\n",
    "lr = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Gradient descent to find the principal component\n",
    "for i in range(iterations):\n",
    "    # Project the data onto the principal component vector\n",
    "    projected = X @ w\n",
    "\n",
    "    # Compute the loss (negative variance)\n",
    "    loss = -torch.var(projected)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w += lr * w.grad\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "\n",
    "# Normalize the principal component vector\n",
    "principal_component = w / torch.norm(w)\n",
    "print(\"Principal Component:\", principal_component.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17206b27",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_np = np.random.rand(100, 1)\n",
    "y_np = 3 * X_np.squeeze() + 2 + 0.1 * np.random.randn(100)\n",
    "\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Learning rate and iterations\n",
    "lr = 0.1\n",
    "iterations = 1000\n",
    "\n",
    "# Gradient descent for linear regression\n",
    "for i in range(iterations):\n",
    "    # Compute predictions\n",
    "    y_pred = X @ w + b\n",
    "\n",
    "    # Compute the loss (mean squared error)\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Learned weights:\", w.item())\n",
    "print(\"Learned bias:\", b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed3a21",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09711dea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
