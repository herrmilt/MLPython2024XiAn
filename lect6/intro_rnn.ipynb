{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "A recurrent neural network contains neurons with feedback loops. They are very good for using serial data.\n",
    "\n",
    "Let see an example. Supose we have sequences of numbers and we want to estimate the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = torch.arange(6).repeat(1000, 1)*torch.randn((1000, 1))\n",
    "Xs = all[:, 0:-1]\n",
    "Ys = all[:,-1]\n",
    "print(Xs.shape, Ys.shape)\n",
    "Xs[:5], Ys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for the inputs. Inputs are used while training one by one\n",
    "input_weights = nn.Linear(1, 1)\n",
    "recurrent_weights = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "print(input_weights.weight), print(input_weights.bias)\n",
    "print(recurrent_weights.weight)\n",
    "\n",
    "batch = Xs[:5]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tanh(input_weights(batch[:, 0].unsqueeze(1)))\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tanh(recurrent_weights(output) + input_weights(batch[:, 1].unsqueeze(1)))\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is repeated for all the inputs, stored each one as a matrix column.\n",
    "\n",
    "We can add more RNN neurons to the system, adding a Linear module to combine the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the operations are differentiable, we can learn from data with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_neurons=5):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.input_weights = nn.Linear(1, num_neurons)\n",
    "        self.recurrent_weights = nn.Linear(num_neurons, num_neurons, bias=False)\n",
    "        self.fc = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch, cols = xs.shape\n",
    "        output = torch.zeros((batch, self.num_neurons))\n",
    "\n",
    "        for i in range(cols):\n",
    "            in_value = xs[:, i].unsqueeze(1)\n",
    "            output = torch.tanh(self.input_weights(in_value) + self.recurrent_weights(output))\n",
    "\n",
    "        return self.fc(output)\n",
    "    \n",
    "model = Model()\n",
    "sum([n.nelement() for n in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(Xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    outputs = model(Xs)\n",
    "    loss = loss_fn(outputs.squeeze(), Ys)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test = torch.arange(6).repeat(200, 1)*torch.randn((200, 1))\n",
    "Xs_test = all_test[:, 0:-1]\n",
    "Ys_test = all_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(Xs_test)\n",
    "    loss = loss_fn(output.squeeze(), Ys_test)\n",
    "print('Testing loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:5], Ys_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, each value in the training dataset contains a single feature, but in general it can contains as much as necessary. For example, consider that we have trajectories in the 2D plane, and we want to calculate the next point based on previous points. In this case, every point is described by 2 features, the X and Y position.\n",
    "\n",
    "Lets first generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trajectory is creating by y = ax + b, with a and b random\n",
    "a = torch.randn((1200, 1))\n",
    "b = torch.randn((1200, 1)) * 5 \n",
    "x = torch.arange(6).repeat((1200, 1))\n",
    "y = a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.stack([x, y], dim=2)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in Xs and Ys\n",
    "Xs = dataset[:, :-1, :]\n",
    "Ys = dataset[:, -1, :]\n",
    "Xs.shape, Ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test\n",
    "Xs_train, Ys_train = Xs[:1000], Ys[:1000]\n",
    "Xs_test, Ys_test = Xs[1000:], Ys[1000:]\n",
    "Xs_train.shape, Ys_train.shape, Xs_test.shape, Ys_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to provide to the RNN network pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "class Model2D(nn.Module):\n",
    "    def __init__(self, num_neurons=10):\n",
    "        super(Model2D, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.input_weights = nn.Linear(2, num_neurons)\n",
    "        self.recurrent_weights = nn.Linear(num_neurons, num_neurons, bias=False)\n",
    "        self.fc = nn.Linear(num_neurons, 2)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch, cols, _ = xs.shape\n",
    "        output = torch.zeros((batch, 1, self.num_neurons))\n",
    "\n",
    "        for i in range(cols):\n",
    "            in_value = xs[:, i, :].unsqueeze(1)\n",
    "            output = torch.tanh(self.input_weights(in_value) + self.recurrent_weights(output))\n",
    "\n",
    "        return self.fc(output).squeeze()\n",
    "    \n",
    "model2d = Model2D()\n",
    "sum([n.nelement() for n in model2d.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated = model2d(Xs_train[:20])\n",
    "estimated.shape, Ys_train[:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "loss_fn(estimated, Ys_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model2d.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model2d.train()\n",
    "\n",
    "    outputs = model2d(Xs_train)\n",
    "    loss = loss_fn(outputs, Ys_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model2d(Xs_test)\n",
    "    loss = loss_fn(output, Ys_test)\n",
    "print('Testing loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:5], Ys_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch contains a module that directly implements the RNN neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "class Model2DStd(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model2DStd, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.device = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        batch_size, _, _ = xs.shape\n",
    "        if self.device is None:\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_size)  \n",
    "        else:\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_size, device=self.device)\n",
    "        out, hn = self.rnn(xs, h0)  \n",
    "        out = self.fc(hn.squeeze(0))  \n",
    "        return out\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "    \n",
    "model2d_std = Model2DStd(2, 10, 2)\n",
    "sum([n.nelement() for n in model2d_std.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated = model2d_std(Xs_train[:20])\n",
    "estimated.shape, Ys_train[:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model2d_std.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model2d_std.train()\n",
    "\n",
    "    outputs = model2d_std(Xs_train)\n",
    "    loss = loss_fn(outputs, Ys_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model2d_std(Xs_test)\n",
    "    loss = loss_fn(output, Ys_test)\n",
    "print('Testing loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of RNN networks\n",
    "\n",
    "Since RNN might contains large sequences of chained operations, the information contained in the first inputs can be forgotten. \n",
    "\n",
    "Lets see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100\n",
    "Xs = torch.randn((1200, SEQ_LENGTH, 1))\n",
    "Ys = Xs[:, -5] + Xs[:, -1]\n",
    "\n",
    "Xs_train, Ys_train = Xs[:1000], Ys[:1000]\n",
    "Xs_test, Ys_test = Xs[1000:], Ys[1000:]\n",
    "Xs_train.shape, Ys_train.shape, Xs_test.shape, Ys_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_add = Model2DStd(1, SEQ_LENGTH, 1)\n",
    "sum([n.nelement() for n in model_add.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_add(Xs_train[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_device = model_add.to(device)\n",
    "Xs_train_device = Xs_train.to(device)\n",
    "Ys_train_device = Ys_train.to(device)\n",
    "Xs_test_device = Xs_test.to(device)\n",
    "Ys_test_device = Ys_test.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_device.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model_device.train()\n",
    "\n",
    "    outputs = model_device(Xs_train_device)\n",
    "    loss = loss_fn(outputs, Ys_train_device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        with torch.no_grad():\n",
    "            output_tst = model_device(Xs_test_device)\n",
    "            loss_test = loss_fn(output_tst, Ys_test_device)\n",
    "\n",
    "        print(epoch, \"Loss train:\", loss.item(), \"Loss test:\", loss_test.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what happen now when the model needs to remember a very early value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100\n",
    "Xs = torch.randn((1200, SEQ_LENGTH, 1))\n",
    "Ys = Xs[:, 0] + Xs[:, -1]\n",
    "\n",
    "Xs_train, Ys_train = Xs[:1000], Ys[:1000]\n",
    "Xs_test, Ys_test = Xs[1000:], Ys[1000:]\n",
    "\n",
    "model_add = Model2DStd(1, 10, 1)\n",
    "\n",
    "model_device = model_add.to(device)\n",
    "Xs_train_device = Xs_train.to(device)\n",
    "Ys_train_device = Ys_train.to(device)\n",
    "Xs_test_device = Xs_test.to(device)\n",
    "Ys_test_device = Ys_test.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_device.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model_device.train()\n",
    "\n",
    "    outputs = model_device(Xs_train_device)\n",
    "    loss = loss_fn(outputs, Ys_train_device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        with torch.no_grad():\n",
    "            output_tst = model_device(Xs_test_device)\n",
    "            loss_test = loss_fn(output_tst, Ys_test_device)\n",
    "\n",
    "        print(epoch, \"Loss train:\", loss.item(), \"Loss test:\", loss_test.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other types of recurrent networks types created to alleviate this problem:\n",
    "- Gated Recurrent Unit (GRU)\n",
    "- Long-Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
