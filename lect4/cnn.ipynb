{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    def __init__(self, num_samples, image_size=28):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.data, self.labels = self.generate_data(num_samples, image_size)\n",
    "        \n",
    "    def generate_data(self, num_samples, image_size):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for _ in range(num_samples):\n",
    "            label = np.random.randint(0, 2)\n",
    "            image = np.zeros((image_size, image_size), dtype=np.float32)\n",
    "            if label == 0:\n",
    "                # Draw a square\n",
    "                side = np.random.randint(5, image_size // 2)\n",
    "                top_left_x = np.random.randint(0, image_size - side)\n",
    "                top_left_y = np.random.randint(0, image_size - side)\n",
    "                image[top_left_x:top_left_x+side, top_left_y:top_left_y+side] = 1.0\n",
    "            else:\n",
    "                # Draw a circle\n",
    "                radius = np.random.randint(5, image_size // 4)\n",
    "                center_x = np.random.randint(radius, image_size - radius)\n",
    "                center_y = np.random.randint(radius, image_size - radius)\n",
    "                y, x = np.ogrid[:image_size, :image_size]\n",
    "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "                image[mask] = 1.0\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "        return torch.tensor(data).unsqueeze(1), torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "train_dataset = ShapesDataset(num_samples=1000)\n",
    "test_dataset = ShapesDataset(num_samples=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    image, label = train_dataset[i]\n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {'Square' if label.item() == 0 else 'Circle'}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a network using pixels as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "\n",
    "# The first step is flattening the image to enter the networn\n",
    "nn.Flatten(1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(784, 20),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this solution:\n",
    "- The interaction of every pair of pixels is taken into account in the linear layer. This creates a very large number of parameters to learn.\n",
    "- The model is very sensitive to image translations, because each pixel is considered a feature.\n",
    "- The locality is completelly missed. Borders, for example, cannot be found.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "A convolution is a mathematical operation that combines two sets of information.\n",
    "\n",
    "In computer vision, a convolution transform an image using a kernel. Convolution transforms pixels into features, that takes into account in a particular way the relations of each pixel with its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_convolution(input_data, kernel):\n",
    "    # Get dimensions\n",
    "    input_h, input_w = input_data.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = input_h - kernel_h + 1\n",
    "    output_w = input_w - kernel_w + 1\n",
    "\n",
    "    # Initialize the output\n",
    "    output = torch.zeros((output_h, output_w), dtype=torch.float32, device=input_data.device)\n",
    "\n",
    "    # Perform the convolution operation\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract the region of interest\n",
    "            region = input_data[i:i+kernel_h, j:j+kernel_w]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            output[i, j] = torch.sum(region * kernel)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, title='',*, cmap='gray', **kwargs):\n",
    "    plt.imshow(image, cmap=cmap, **kwargs)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "image = np.zeros((10, 10), dtype=np.float32)\n",
    "image[3:7, 3:7] = 1.0\n",
    "\n",
    "# Display the original image\n",
    "display_image(image, title='Original Image')\n",
    "\n",
    "# Convert the image to a PyTorch tensor and add a batch dimension and a channel dimension\n",
    "image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_by_kernel(image_tensor, kernel, title):\n",
    "    output = F.conv2d(image_tensor, kernel, padding='same').squeeze().numpy()\n",
    "    display_image(output, title)\n",
    "\n",
    "edge_detection_kernel = torch.tensor([[\n",
    "    [-1, -1, -1], \n",
    "    [-1, 8, -1], \n",
    "    [-1, -1, -1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, edge_detection_kernel, 'Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edge_kernel = torch.tensor([[\n",
    "    [-1, 0, 1], \n",
    "    [-1, 0, 1], \n",
    "    [-1, 0, 1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, vertical_edge_kernel, 'Horizontal Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_edge_kernel = torch.tensor([[\n",
    "    [1, 1, 1], \n",
    "    [0, 0, 0], \n",
    "    [-1, -1, -1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, horizontal_edge_kernel, 'Horizontal Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_kernel = torch.tensor([[\n",
    "    [0, -1, 0], \n",
    "    [-1, 4, -1], \n",
    "    [0, -1, 0]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, corner_kernel, 'Corners')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions find new features that contains information a pixel region, so now they can include information about borders.\n",
    "\n",
    "If we train a network using convoluted values, we can provide better information with less size:\n",
    "- Learn the kernels in the first layers\n",
    "\n",
    "Additionally, we would need to reduce the size of the images on each layer:\n",
    "- Reduce the dimensionality, so the number of parameters\n",
    "- Translation invariance, by summarizing feautures in regions\n",
    "- Hierarchical features, by producing more abstract features per layers\n",
    "- Noise reduction \n",
    "\n",
    "CNNs are neural networks that use convolutions to create higher-level features based on the input images:\n",
    "- In convolutional layers, the kernels are learnt\n",
    "- All operations are differenciable, so can be learned by gradient descent and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(3 * 3, 2)  # Adjust the dimensions accordingly\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_1 = [img for img, cls in train_dataset if cls == 0][0]\n",
    "circle_1 = [img for img, cls in train_dataset if cls == 1][0]\n",
    "display_image(square_1.squeeze())\n",
    "display_image(circle_1.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.conv1(square_1).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.conv1(circle_1).squeeze()\n",
    "    display_image(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.conv2(square_1).squeeze()\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.conv2(model.pool(torch.tanh(model.conv1(square_1)))).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.conv2(model.pool(torch.tanh(model.conv1(circle_1)))).squeeze()\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.pool(torch.tanh(model.conv2(\n",
    "        model.pool(torch.tanh(model.conv1(square_1)))))).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.pool(torch.tanh(model.conv2(\n",
    "        model.pool(torch.tanh(model.conv1(circle_1)))))).squeeze()\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = [img for img, cls in train_dataset if cls == 0][:6]\n",
    "circles = [img for img, cls in train_dataset if cls == 1][:6]\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(5, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = model.pool(torch.tanh(model.conv2(\n",
    "            model.pool(torch.tanh(model.conv1(squares[i])))))).squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(5, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = model.pool(torch.tanh(model.conv2(\n",
    "            model.pool(torch.tanh(model.conv1(circles[i])))))).squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real, more complex applications, we usually performs different parallel convolutions to the same input. The result of each of the convolutions is stored in a different chanel of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple kernels\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple channels\n",
    "\n",
    "image = torch.randn((1, 3, 28, 28))\n",
    "nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple channels and kernels\n",
    "\n",
    "image = torch.randn((1, 3, 28, 28))\n",
    "nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel size\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7, padding=3)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding, auto\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "print(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding='same')(image).shape)\n",
    "print(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7, padding='same')(image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets change the model to use multiple kernels per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(16 * 3 * 3, 2)  # out_channels * (28 // 3 //3) * (28 // 3 //3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 3 * 3)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In color images, the input images has three channels, so we need to adjust the conv2d parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 0 are yellow squares, class 1 are red squares and yellow circles\n",
    "# channels are R, G, B. yellow = R+G\n",
    "\n",
    "def create_square(image_size, color='rgb'):\n",
    "    image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "    side = np.random.randint(5, image_size // 2)\n",
    "    top_left_x = np.random.randint(0, image_size - side)\n",
    "    top_left_y = np.random.randint(0, image_size - side)\n",
    "    for idx, c in enumerate('rgb'):\n",
    "        if c in color:\n",
    "            image[idx, top_left_x:top_left_x+side, top_left_y:top_left_y+side] = 1.0\n",
    "    return image\n",
    "\n",
    "def create_circle(image_size, color='rgb'):\n",
    "    image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "    radius = np.random.randint(5, image_size // 4)\n",
    "    center_x = np.random.randint(radius, image_size - radius)\n",
    "    center_y = np.random.randint(radius, image_size - radius)\n",
    "    y, x = np.ogrid[:image_size, :image_size]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "    for idx, c in enumerate('rgb'):\n",
    "        if c in color:\n",
    "            image[idx, mask] = 1.0\n",
    "    return image\n",
    "\n",
    "class ShapesDataset2(Dataset):\n",
    "    def __init__(self, num_samples, image_size=28):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.data, self.labels = self.generate_data(num_samples, image_size)\n",
    "        \n",
    "    def generate_data(self, num_samples, image_size):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for _ in range(num_samples):\n",
    "            label = np.random.randint(0, 2)\n",
    "            # image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "            if label == 0:\n",
    "                # Draw a yellow square\n",
    "                image = create_square(image_size, 'rg')\n",
    "            else:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    image = create_circle(image_size, 'rg')\n",
    "                else:\n",
    "                    image = create_square(image_size, 'r')\n",
    "                    \n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "        return torch.tensor(data), torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "train_dataset = ShapesDataset2(num_samples=1000)\n",
    "test_dataset = ShapesDataset2(num_samples=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some samples\n",
    "fig, axes = plt.subplots(3, 5, figsize=(10,6))\n",
    "for i in range(15):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    ax = axes[row, col]\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {'Class 1' if label.item() == 0 else 'Class 2'}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNColor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNColor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(2 * 3 * 3, 2)  # out_channels * (28 // 3 //3) * (28 // 3 //3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(-1, 2 * 3* 3)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNNColor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_channels, in_channels, h, w\n",
    "model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.conv1.weight[0])\n",
    "torch.sum(model.conv1.weight[0], dim=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.conv1.weight[1])\n",
    "torch.sum(model.conv1.weight[1], dim=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_color(image, title='',*, cmap='gray'):\n",
    "    plt.imshow(image.squeeze().permute(1, 2, 0).numpy(), cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "obj = torch.tensor(create_square(28,'rg'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = torch.tensor(create_square(28,'r'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = torch.tensor(create_circle(28,'rg'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
