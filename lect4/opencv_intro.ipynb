{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c0e61",
   "metadata": {
    "id": "cIU7W8Wmebey"
   },
   "source": [
    "# Introduction to OpenCV\n",
    "\n",
    "## Getting started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5cf0cd",
   "metadata": {
    "id": "TXKxw8iJebez"
   },
   "outputs": [],
   "source": [
    "# These imports let you use opencv\n",
    "import cv2 #opencv itself\n",
    "import numpy as np # matrix manipulations\n",
    "\n",
    "#the following are to do with this interactive notebook code\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt # this lets you draw inline pictures in the notebooks\n",
    "import pylab # this allows you to control figure size\n",
    "pylab.rcParams['figure.figsize'] = (5, 5) # this controls figure size in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11346c",
   "metadata": {
    "id": "htK6mm-Gebe2"
   },
   "source": [
    "\n",
    "\n",
    "Now we can open an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05c2ba",
   "metadata": {
    "id": "Ah762ATHebe3"
   },
   "outputs": [],
   "source": [
    "input_image=cv2.imread('images/noidea.jpg')\n",
    "type(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c150b",
   "metadata": {
    "id": "ut1_Lwdgebe5"
   },
   "source": [
    "We can find out various things about that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c240df1",
   "metadata": {
    "id": "awdTYn4Gebe6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(input_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf502692",
   "metadata": {
    "id": "af7iQyhqebe8"
   },
   "outputs": [],
   "source": [
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0229ed2",
   "metadata": {
    "id": "UhxrodZrebe_"
   },
   "outputs": [],
   "source": [
    "print(input_image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed38452",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"image\", input_image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f2c99",
   "metadata": {
    "id": "woP9RhyCebfB"
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9db107",
   "metadata": {
    "collapsed": true,
    "id": "6VFxWhvUebfD"
   },
   "source": [
    "**Note**. Here another issue with conventions. OpenCV expect images to be stored as BGR, but matplotlib expected to be RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6882b",
   "metadata": {
    "id": "zgEQX0isebfD"
   },
   "outputs": [],
   "source": [
    "# split channels\n",
    "b,g,r=cv2.split(input_image)\n",
    "# show one of the channels (this is red - see that the sky is kind of dark. try changing it to b)\n",
    "plt.imshow(b, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80655d86",
   "metadata": {
    "id": "XqE1jCKaebfG"
   },
   "source": [
    "## converting between colour spaces, merging and splitting channels\n",
    "\n",
    "We can convert between various colourspaces in OpenCV easily. We've seen how to split, above. We can also merge channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217098b",
   "metadata": {
    "id": "Ev_3hJKLebfH"
   },
   "outputs": [],
   "source": [
    "image=cv2.merge([r,g,b])\n",
    "# merge takes an array of single channel matrices\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e63f1",
   "metadata": {
    "id": "cJ-UCAynebfJ"
   },
   "source": [
    "OpenCV also has a function specifically for dealing with image colorspaces, so rather than split and merge channels by hand you can use this instead. It is usually marginally faster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa775b",
   "metadata": {
    "id": "egPmVUvYebfK"
   },
   "outputs": [],
   "source": [
    "COLORflags = [flag for flag in dir(cv2) if flag.startswith('COLOR') ]\n",
    "print(len(COLORflags))\n",
    "\n",
    "# If you want to see them all, rather than just a count uncomment the following line\n",
    "# print(COLORflags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d269d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(10, 6), cmap=\"gray\"):\n",
    "    if titles is None:\n",
    "        titles = \" \" * len(images)\n",
    "    fig, axes = plt.subplots(len(images), 1, figsize=figsize)\n",
    "\n",
    "    for i, (image, title) in enumerate(zip(images, titles)):\n",
    "        axes[i].imshow(image, cmap)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff990d",
   "metadata": {
    "id": "INRZEZdvebfM"
   },
   "outputs": [],
   "source": [
    "hsv_image=cv2.cvtColor(image, cv2.COLOR_BGR2YCR_CB)\n",
    "x, y, z =cv2.split(hsv_image)\n",
    "plot_images([x, y, z], [\"Y\", \"Cr\", \"Cb\"], (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7bac4",
   "metadata": {
    "id": "KsFd9SBzebfS"
   },
   "source": [
    "## Getting and setting regions of an image\n",
    "\n",
    "In the same way as we can get or set individual pixels, we can get or set regions of an image. This is a particularly useful way to get a region of interest to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d2fde",
   "metadata": {
    "id": "D0rwsf8sebfS"
   },
   "outputs": [],
   "source": [
    "dogface = image[60:250, 70:350]\n",
    "plt.imshow(dogface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292a97c",
   "metadata": {
    "id": "i1xm1L5MebfU"
   },
   "outputs": [],
   "source": [
    "fresh_image=image.copy()\n",
    "fresh_image[200:200+dogface.shape[0], 200:200+dogface.shape[1]]=dogface\n",
    "print(dogface.shape[0])\n",
    "print(dogface.shape[1])\n",
    "plt.imshow(fresh_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025e073",
   "metadata": {
    "collapsed": true,
    "id": "bCVZFlDhebfW"
   },
   "source": [
    "## Matrix slicing\n",
    "In OpenCV python style, as I have mentioned, images are numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fd99f",
   "metadata": {
    "id": "9LLhUPE7ebfX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freshim2 = image.copy()\n",
    "crop = freshim2[100:400, 130:300]\n",
    "plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09aecc",
   "metadata": {
    "id": "9cSa7WDHebfZ"
   },
   "outputs": [],
   "source": [
    "hsvim=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "bcrop =hsvim[100:400, 100:300, 1]\n",
    "plt.imshow(bcrop, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff574f",
   "metadata": {},
   "source": [
    "## Image stats and image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d05f0",
   "metadata": {
    "id": "b8PY3kZ63fWO"
   },
   "source": [
    "### Basic manipulations\n",
    "\n",
    "Rotate, flip..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21496",
   "metadata": {
    "id": "4LHzdNvt3fWP"
   },
   "outputs": [],
   "source": [
    "flipped_code_0=cv2.flip(image,0) # vertical flip\n",
    "plt.imshow(flipped_code_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3e4ea",
   "metadata": {
    "id": "9SOq_oD-3fWR"
   },
   "outputs": [],
   "source": [
    "flipped_code_1=cv2.flip(image,1) # horizontal flip\n",
    "plt.imshow(flipped_code_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe5314",
   "metadata": {
    "id": "7zapvC1p3fWU"
   },
   "outputs": [],
   "source": [
    "transposed=cv2.transpose(image)\n",
    "plt.imshow(transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc46725",
   "metadata": {
    "id": "qF_AsupK3fWa"
   },
   "source": [
    "### Arithmetic operations on images\n",
    "\n",
    "OpenCV has a lot of functions for doing mathematics on images.\n",
    "- Usually better that doing matrix operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99b3a0",
   "metadata": {
    "id": "Gg5M59Lt3fWa"
   },
   "outputs": [],
   "source": [
    "#First create an image the same size as our input\n",
    "blank_image = np.zeros((input_image.shape), np.uint8)\n",
    "\n",
    "blank_image[100:200,100:200,1]=100; #give it a green square\n",
    "\n",
    "new_image=cv2.add(blank_image,input_image) # add the two images together\n",
    "\n",
    "plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f131c",
   "metadata": {
    "id": "ke-afLFp3fWd"
   },
   "source": [
    "### Noise reduction\n",
    "Noise reduction usually involves blurring/smoothing an image using a Gaussian kernel.\n",
    "The width of the kernel determines the amount of smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e865aa",
   "metadata": {
    "id": "EyxmwP0E3fWd"
   },
   "outputs": [],
   "source": [
    "d=3\n",
    "img_blur3 = cv2.GaussianBlur(image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "plot_images([image, img_blur3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f461f",
   "metadata": {
    "id": "GjGY7Dl33fWg"
   },
   "outputs": [],
   "source": [
    "d=5\n",
    "blured = cv2.GaussianBlur(image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "plt.imshow(blured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f50b88",
   "metadata": {
    "id": "AaJ7zd1w3fWi"
   },
   "outputs": [],
   "source": [
    "d=20\n",
    "blured = cv2.GaussianBlur(image, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "plt.imshow(blured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb9673",
   "metadata": {
    "id": "Hp50ZT_A3fWk"
   },
   "source": [
    "### Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906297f",
   "metadata": {
    "id": "d7ceQSv13fWk"
   },
   "outputs": [],
   "source": [
    "sobelimage=cv2.cvtColor(input_image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sobelx = cv2.Sobel(sobelimage,cv2.CV_64F,1,0,ksize=9)\n",
    "sobely = cv2.Sobel(sobelimage,cv2.CV_64F,0,1,ksize=9)\n",
    "plt.imshow(sobelx,cmap = 'gray')\n",
    "# Sobel works in x and in y, change sobelx to sobely in the olt line above to see the difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4b8eb",
   "metadata": {
    "id": "RWxFMGQe3fWm"
   },
   "source": [
    "Canny edge detection is another winnning technique - it takes two thresholds.\n",
    "The first one determines how likely Canny is to find an edge, and the second determines how likely it is to follow that edge once it's found. Investigate the effect of these thresholds by altering the values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8ac3",
   "metadata": {
    "id": "1MJQRgXL3fWn"
   },
   "outputs": [],
   "source": [
    "th1=30\n",
    "th2=60 # Canny recommends threshold 2 is 3 times threshold 1 - you could try experimenting with this...\n",
    "d=3 # gaussian blur\n",
    "\n",
    "edgeresult=image.copy()\n",
    "edgeresult = cv2.GaussianBlur(edgeresult, (2*d+1, 2*d+1), -1)[d:-d,d:-d]\n",
    "\n",
    "edge = cv2.Canny(edgeresult, th1, th2)\n",
    "\n",
    "edgeresult[edge != 0] = (0, 255, 0) # this takes pixels in edgeresult where edge non-zero colours them bright green\n",
    "\n",
    "plt.imshow(edgeresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1f57e",
   "metadata": {
    "id": "X7HD5UmG5NY-"
   },
   "source": [
    "## Features in computer vision\n",
    "\n",
    "Features are image locations that are \"easy\" to find in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d7f25",
   "metadata": {
    "id": "yW11cMb15NZB"
   },
   "source": [
    "### Corner detectors\n",
    "If you think of edges as being lines, then corners are an obvious choice for features as they represent the intersection of two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886855f",
   "metadata": {
    "id": "JdU2qm635NZC"
   },
   "outputs": [],
   "source": [
    "harris_test=input_image.copy()\n",
    "#greyscale it\n",
    "gray = cv2.cvtColor(harris_test,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "blocksize=4 #\n",
    "kernel_size=3 # sobel kernel: must be odd and fairly small\n",
    "\n",
    "# run the harris corner detector\n",
    "dst = cv2.cornerHarris(gray,blocksize,kernel_size,0.05) # parameters are blocksize, Sobel parameter and Harris threshold\n",
    "\n",
    "#result is dilated for marking the corners, this is visualisation related and just makes them bigger\n",
    "dst = cv2.dilate(dst,None)\n",
    "#we then plot these on the input image for visualisation purposes, using bright red\n",
    "harris_test[dst>0.01*dst.max()]=[0,0,255]\n",
    "plt.imshow(cv2.cvtColor(harris_test, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a057e25",
   "metadata": {
    "collapsed": true,
    "id": "0_6wW-nY5NZH"
   },
   "source": [
    "## Moving towards feature space\n",
    "When we consider modern feature detectors there are a few things we need to mention. What makes a good feature includes the following:\n",
    "\n",
    "* Repeatability (got to be able to find it again)\n",
    "* Distinctiveness/informativeness (features representing different things need to be different)\n",
    "* Locality (they need to be local to the image feature and not, like, the whole image)\n",
    "* Quantity (you need to be able to find enough of them for them to be properly useful)\n",
    "* Accuracy (they need to accurately locate the image feature)\n",
    "* Efficiency (they've got to be computable in reasonable time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaad6a7",
   "metadata": {
    "id": "DXu8K_FJ5NZI"
   },
   "outputs": [],
   "source": [
    "orbimg=input_image.copy()\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(orbimg,None)\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(orbimg, kp)\n",
    "# draw keypoints\n",
    "cv2.drawKeypoints(orbimg,kp,orbimg)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.imshow(cv2.cvtColor(orbimg, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63244cf7",
   "metadata": {
    "collapsed": true,
    "id": "tynAouFx5NZK"
   },
   "source": [
    "## Matching features\n",
    "Finding features is one thing but actually we want to use them for matching.\n",
    "First let's get something where we know there's going to be a match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f467b1",
   "metadata": {
    "id": "xyjEKvnK5NZK"
   },
   "outputs": [],
   "source": [
    "img2match=np.zeros(input_image.shape,np.uint8)\n",
    "dogface=input_image[60:250, 70:350] # copy out a bit\n",
    "img2match[60:250,70:350]=[0,0,0] # blank that region\n",
    "dogface=cv2.flip(dogface,0) #flip the copy\n",
    "img2match[200:200+dogface.shape[0], 200:200+dogface.shape[1]]=dogface # paste it back somewhere else\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img2match, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d8250",
   "metadata": {
    "id": "-pzjjdcD5NZN"
   },
   "source": [
    "## Matching keypoints\n",
    "\n",
    "The feature matching function (in this case Orb) detects and then computes keypoint descriptors. These are a higher dimensional representation of the image region immediately around a point of interest (sometimes literally called \"interest points\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af54e3",
   "metadata": {
    "id": "a8pk6NeP5NZN"
   },
   "outputs": [],
   "source": [
    "\n",
    "kp2 = orb.detect(img2match,None)\n",
    "# compute the descriptors with ORB\n",
    "kp2, des2 = orb.compute(img2match, kp2)\n",
    "# create BFMatcher object: this is a Brute Force matching object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "# Match descriptors.\n",
    "matches = bf.match(des,des2)\n",
    "\n",
    "# Sort them by distance between matches in feature space - so the best matches are first.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 50 matches.\n",
    "oimg = cv2.drawMatches(orbimg,kp,img2match,kp2,matches[:50], orbimg)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(oimg, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd26e32",
   "metadata": {
    "id": "E2qdpBTV5NZP"
   },
   "source": [
    "As you can see there are some false matches, but it's fairly clear that most of the matched keypoints found are actual matches between image regions on the dogface.\n",
    "\n",
    "To be more precise about our matching we could choose to enforce **homography** constraints, which looks for features than sit on the same plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6ffde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
